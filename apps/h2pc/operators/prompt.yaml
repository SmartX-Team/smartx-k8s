{{- $ollama := .params.ollama | default false }}
---
kind: LLM
params:
  - name: modelName
    type: String
    default: exaone-deep:32b
  - name: name
    type: String
  - name: ollama
    type: Boolean
    default: false
env:
{{- if $ollama }}
  - name: OLLAMA_BASE_URL
    value: http://localhost:11434
  - name: OLLAMA_MODEL_NAME
    value: {{ .params.modelName | quote }}
{{- else }}
  - name: OPENAI_MODEL_NAME
    value: {{ .params.modelName | quote }}
{{- end }}
features:
  ollama: {{ $ollama }}
resources: null
srcs: null
sinks:
  - kind: Stream # Options: [Stream]
template: |

{{- if not ( hasKey .params "name" ) }}
{{- fail ( printf "Missing prompt name parameter: %s" .name ) }}
{{- end }}

{{- $context := dict
  "name" .params.name
  "template" nil
}}

{{- /* Load a prompt template from extra prompts */}}
{{- range $_ := .ExtraPrompts }}
{{- if eq .name $context.name }}
{{- if empty $context.template }}
{{- $_ := set $context "template" ( .template | toYaml ) }}
{{- else }}
{{- fail ( printf "Duplicated prompt: %s" $context.name ) }}
{{- end }}
{{- end }}
{{- end }}

{{- /* Load a prompt template from builtin prompts */}}
{{- if empty $context.template }}
{{- $filePath := printf "prompts/%s.yaml" $context.name }}
{{- if not ( empty ( $.Files.Glob $filePath ) ) }}
{{- $_ := set $context "template" ( $.Files.Get $filePath | trim ) }}
{{- else }}
{{- fail ( printf "No such prompt: %s" $context.name ) }}
{{- end }}
{{- end }}

{{- /* Render a prompt template */}}
{{- $context.template | nindent 2 }}
